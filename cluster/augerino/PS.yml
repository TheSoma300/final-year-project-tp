apiVersion: batch/v1
kind: Job
metadata:
  name: tpproject-augerino-ps-50-amptoDB
  namespace: 2267217fproject
# this is where you define the content of the Job
spec:
  parallelism: 1
  completions: 1
  template:        
    metadata:
      name: tpproject
    spec:
      containers:
      - name: tpproject-training  
        image: somaf/tp-project:train-70ebcd179dab41a0a013f5e33c616cf11a6f9c7d
        command: 
          - "python"
        args:
          - "runner.py"
          - "--config"
          - "augerino"
          - "--data-path"
          - "/nfs/data"
          - "--checkpoint"
          - "/nfs/checkpoints"
          - "-t"
          - "ps"
          - "--title"
          - "Augerino - 50 epochs - Pitch Shift"
          - "-e"
          - "50"
        resources:
          requests:
            # the "m" suffix here means "millicores", so 1 physical CPU
            # core = 1000m. this container requests 2000m = 2 physical cores
            cpu: "2000m" 
            # memory units are also defined by a suffix. typically this will
            # be "Mi" or "Gi" as appropriate
            memory: "16Gi"
            # GPUs are slightly different as they're not natively supported
            # by Kubernetes. This indicates that the container requires 1 
            # GPU in order to run
            nvidia.com/gpu: 1 
          limits:
            cpu: "4000m" 
            memory: "16Gi"
            nvidia.com/gpu: 1
        workingDir: /app
        volumeMounts:
        - mountPath: /nfs
          name: nfs-access
        env:
        - name: NUMBA_CACHE_DIR
          value: /tmp 
      volumes:
      - name: nfs-access
        persistentVolumeClaim:
          claimName: 2267217fvol1claim 
      # in some cases you will want to run your job on a node with a specific type of
      # GPU. the nodeSelector section allows you to do this. The compute nodes each
      # have an annotation indicating the type of GPU they contain. The 2 lines below
      # tell the Kubernetes scheduler that this job must be scheduled on a node
      # where the value of the "node-role.ida/gpu2080ti" annotation is true, i.e. on
      # a node with RTX 2080 Ti GPUs. To do the equivalent for the RTX Titan nodes, 
      # change "gpu2080ti" to "gputitan"
      # nodeSelector:
      #   node-role.ida/gpu3090: "true"
      # determines what Kubernetes will do if the container inside the 
      # pod fails to start or crashes. This just tells it to give up
      # without retrying.
      restartPolicy: Never
